{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878c37b3-db56-4113-a273-b5eb5c63a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Combinations by Overall Rank:\n",
      "                Dataset                       Model  Accuracy  Precision  \\\n",
      "18       Normalizer_IQR      RandomForestClassifier  0.539919   0.536365   \n",
      "21   StandardScaler_IQR      DecisionTreeClassifier  0.534506   0.533239   \n",
      "22   StandardScaler_IQR      RandomForestClassifier  0.535859   0.531831   \n",
      "17       Normalizer_IQR      DecisionTreeClassifier  0.527740   0.524488   \n",
      "11  MinMaxScaler_ZScore  GradientBoostingClassifier  0.507363   0.503708   \n",
      "\n",
      "      Recall  F1 Score  Accuracy_rank  Precision_rank  Recall_rank  F1_rank  \\\n",
      "18  0.539919  0.533849            1.0             1.0          1.0      1.0   \n",
      "21  0.534506  0.529345            3.0             2.0          3.0      2.0   \n",
      "22  0.535859  0.527997            2.0             3.0          2.0      3.0   \n",
      "17  0.527740  0.522468            4.0             4.0          4.0      4.0   \n",
      "11  0.507363  0.495151            7.5             7.5          7.5     11.5   \n",
      "\n",
      "    Total_rank  \n",
      "18         1.0  \n",
      "21         2.5  \n",
      "22         2.5  \n",
      "17         4.0  \n",
      "11         8.5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to evaluate classification models\n",
    "def evaluate_classification_model(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Make predictions\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Define dataset paths\n",
    "datasets = {\n",
    "    'StandardScaler_ZScore': 'termproject/preprocessed_data_standardized.csv',\n",
    "    'Normalizer_ZScore': 'termproject/preprocessed_data_normalized.csv',\n",
    "    'MinMaxScaler_ZScore': 'termproject/preprocessed_data_2.csv',\n",
    "    'MinMaxScaler_LabelEncoder': 'termproject/preprocessed_data.csv',\n",
    "    'Normalizer_IQR': 'termproject/preprocessed_data_normalized_iqr.csv',\n",
    "    'StandardScaler_IQR': 'termproject/preprocessed_data_standard_iqr.csv',\n",
    "}\n",
    "\n",
    "# Define models to be used\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each combination of dataset and model\n",
    "for dataset_name, filepath in datasets.items():\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filepath} not found.\")\n",
    "        continue\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = data.drop(columns=['salary_in_usd'])\n",
    "    y = data['salary_in_usd']\n",
    "    \n",
    "    # Convert target variable to categorical (for classification)\n",
    "    y = pd.qcut(y, q=3, labels=[0, 1, 2])\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        accuracy, precision, recall, f1 = evaluate_classification_model(X_train, X_test, y_train, y_test, model)\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Rank each metric\n",
    "results_df['Accuracy_rank'] = results_df['Accuracy'].rank(ascending=False)  # Accuracy: higher is better\n",
    "results_df['Precision_rank'] = results_df['Precision'].rank(ascending=False)  # Precision: higher is better\n",
    "results_df['Recall_rank'] = results_df['Recall'].rank(ascending=False)  # Recall: higher is better\n",
    "results_df['F1_rank'] = results_df['F1 Score'].rank(ascending=False)  # F1 Score: higher is better\n",
    "\n",
    "# Calculate total rank\n",
    "results_df['Total_rank'] = results_df[['Accuracy_rank', 'Precision_rank', 'Recall_rank', 'F1_rank']].mean(axis=1)\n",
    "\n",
    "# Select the top 5 combinations by overall rank\n",
    "top_5_overall = results_df.nsmallest(5, 'Total_rank')\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 Combinations by Overall Rank:\")\n",
    "print(top_5_overall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e37426-75f5-4aa4-b63d-7f0a927d2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
